Metadata-Version: 2.4
Name: seed_pipeline
Version: 0.1.0
Summary: Seed/Variation ingestion pipeline with API.
Author-email: Ahmed Hammad <ahmedhammad708@yahoo.com>
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: SQLAlchemy>=2.0.0
Requires-Dist: psycopg2-binary>=2.9.0
Requires-Dist: click>=8.1.0
Requires-Dist: fastapi>=0.103.0
Requires-Dist: uvicorn>=0.22.0
Requires-Dist: pydantic>=2.0.0
Requires-Dist: python-dotenv>=1.0.0
Provides-Extra: test
Requires-Dist: pytest; extra == "test"
Requires-Dist: factory-boy; extra == "test"

# Seed Pipeline

This repository implements a simple yet extensible data ingestion pipeline and REST API for
tracking seed names and their observed variations.  It was designed as a
reference implementation for a proof‑of‑concept system but the patterns used
are applicable to production workloads.

## Features

* **Database schema**: Uses PostgreSQL via SQLAlchemy 2.0.  Tables are defined
  for seeds, variations, observations, miners, batches and source files.  The
  schema enforces uniqueness constraints and indexes to support fast lookup.
* **Deterministic normalisation**: Seed and variation strings are
  normalised (lowercased, stripped of accents, collapsed whitespace) and
  hashed so that semantically equivalent inputs map to the same row.  This
  enables idempotent ingestion and easy deduplication.
* **Pluggable adapters**: JSON source files can come in many formats.  A
  registry of adapters converts arbitrary shapes into a canonical record
  (`seed`, `variation`, `miner_ext_id`, `score`, `raw`) which the core
  pipeline consumes.  Adding support for a new format is a matter of
  implementing a new adapter module.
* **Batch ingestion**: The `ingest_batch` function will read a collection of
  files, skip those that have already been processed, and update the
  database accordingly.  A command line wrapper built on `click` is provided
  for convenience (`sp ingest`).
* **REST API**: A small FastAPI application exposes two endpoints: one to
  determine whether seeds exist and how many variations they have, and
  another to classify candidate variations as either existing or new.

## Quick start

You'll need a local PostgreSQL database.  The provided `docker-compose.yml`
starts an instance suitable for development:

```sh
make db
```

Install dependencies into a virtual environment and run the database
migrations:

```sh
make setup
make migrate
```

To ingest a folder of JSON files:

```sh
sp ingest --batch-name demo-2025-08-27 --adapter generic_responses_v1 --path ./samples
```

Start the API locally:

```sh
make api
```

Then query the endpoints:

```sh
curl -X POST http://localhost:8000/seeds/exists -H "Content-Type: application/json" -d '{"seeds": ["Michael", "Ahmed"]}'
curl -X POST http://localhost:8000/seeds/diff -H "Content-Type: application/json" -d '{"seed": "Michael", "variations": ["Mikel", "Micheal", "Mychal"]}'
```

## Licence

This code is provided for educational purposes and comes with no warranty.
